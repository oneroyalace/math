\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{textcomp}

\usepackage{url}

\usepackage{geometry}
\newgeometry{left=12mm, right=17mm, top= 12mm, bottom=12mm}


\usepackage{graphicx}
\usepackage{float}
\usepackage[usenames,dvipsnames]{xcolor}

% Reset line counter for each align environment
\usepackage{etoolbox}
\AtBeginEnvironment{align}{\setcounter{equation}{0}}

\usepackage{amsmath, amsfonts, mathtools, amsthm, amssymb}
\usepackage{mathrsfs}
\usepackage{cancel}

\newcommand\N{\ensuremath{\mathbb{N}}}
\newcommand\R{\ensuremath{\mathbb{R}}}
\newcommand\Z{\ensuremath{\mathbb{Z}}}
\renewcommand\O{\ensuremath{\emptyset}}
\newcommand\Q{\ensuremath{\mathbb{Q}}}
\newcommand\C{\ensuremath{\mathbb{C}}}
\let\implies\Rightarrow
\let\impliedby\Leftarrow
\let\iff\Leftrightarrow
\let\epsilon\varepsilon

% horizontal rule
\newcommand\hr{
    \noindent\rule[0.5ex]{\linewidth}{0.5pt}
}

\usepackage{tikz}
\usepackage{tikz-cd}

% theorems
\usepackage{thmtools}
\usepackage[framemethod=TikZ]{mdframed}
\mdfsetup{skipabove=1em,skipbelow=0em, innertopmargin=5pt, innerbottommargin=6pt}

\theoremstyle{definition}
\declaretheoremstyle[headfont=\bfseries\sffamily, bodyfont=\normalfont, mdframed={ nobreak } ]{thmgreenbox}
\declaretheoremstyle[headfont=\bfseries\sffamily, bodyfont=\normalfont, mdframed={ nobreak } ]{thmredbox}
\declaretheoremstyle[headfont=\bfseries\sffamily, bodyfont=\normalfont]{thmbluebox}
\declaretheoremstyle[headfont=\bfseries\sffamily, bodyfont=\normalfont]{thmblueline}
\declaretheoremstyle[headfont=\bfseries\sffamily, bodyfont=\normalfont, numbered=no, mdframed={ rightline=false, topline=false, bottomline=false, }, qed=\qedsymbol ]{thmproofbox}
\declaretheoremstyle[headfont=\bfseries\sffamily, bodyfont=\normalfont, numbered=no, mdframed={ nobreak, rightline=false, topline=false, bottomline=false } ]{thmexplanationbox}
\declaretheorem[style=thmgreenbox, name=Definition]{definition}
\declaretheorem[sibling=definition, style=thmredbox, name=Corollary]{corollary}
\declaretheorem[sibling=definition, style=thmredbox, name=Proposition]{prop}
\declaretheorem[sibling=definition, style=thmredbox, name=Theorem]{theorem}
\declaretheorem[sibling=definition, style=thmredbox, name=Lemma]{lemma}
\declaretheorem[numbered=no, style=thmexplanationbox, name=Proof]{explanation}
\declaretheorem[numbered=no, style=thmproofbox, name=Proof]{replacementproof}
\declaretheorem[style=thmbluebox,  numbered=no, name=Exercise]{ex}
\declaretheorem[style=thmbluebox,  numbered=no, name=Example]{eg}
\declaretheorem[style=thmblueline, numbered=no, name=Remark]{remark}
\declaretheorem[style=thmblueline, numbered=no, name=Note]{note}


\usepackage{import}
\usepackage{xifthen}
\usepackage{pdfpages}
\usepackage{transparent}
\usepackage{graphicx}

% include figure stored at ./figures/name.pdf_tex
\newcommand{\incfig}[1]{
  \def\svgwidth{0.5\columnwidth}
  \import{./figures/}{#1.pdf_tex}
}


% Matrix with row vectors (see https://tex.stackexchange.com/a/454734)
\newcommand{\brows}[1]{%
  \begin{bmatrix}
  \begin{array}{@{\protect\rotvert\;}c@{\;\protect\rotvert}}
  #1
  \end{array}
  \end{bmatrix}
}
% rotated vertical line (used for matrix w/ row vectors)
\newcommand{\rotvert}{\rotatebox[origin=c]{90}{$\vert$}}
% vertical dot rows in a matrix
\newcommand{\rowsvdots}{\multicolumn{1}{@{}c@{}}{\vdots}}

\title{%
  Math 221 Lec 11  \\
  \large{2.4: Elementary matrices, 2.5: Transpose}
}
\author{Asa Royal (ajr74)}
\date{October 3, 2023}

\begin{document}
\maketitle

\section{Transpose}
\begin{remark}
  The transpose of a matrix \( A \) is the matrix \( A^{\intercal} \) where \( (A^{\intercal})_{ij} = A_{ji} \). Entries are reflected over the diagonal
\end{remark}

\begin{remark}
  If \( x,y \in \mathbb{R}^n \), we can think of their dot dot product \( \textbf{x} \cdot \textbf{y} \) as 
  \begin{equation*}
    \textbf{x} \cdot \textbf{y} = \textbf{x}^{\intercal} \textbf{y} 
  \end{equation*}
\end{remark}

\begin{remark}
  The remark above reveals that vectors themselves are linear transformations. \( \textbf{x} \in \mathbb{R}^n\) is \( n \)-dimensional, but \( \textbf{x} \cdot \textbf{y} \) is 1-dimesional, so \( x^{\intercal} \) is the \( 1 \times n \) matrix that encodes a linear transformation that takes an \( n \)-dimensional vector to 1 dimension. \( \textbf{x}_1 \) tells us what the linear transformation maps \( \hat i \) to, and so on... Add all that hat vectors and you get a scalar: the output of the linear transformation. 
\end{remark}

\begin{definition}[dual]
  The \textbf{dual} of a vector is the linear transformation it encodes. The \textbf{dual} of a linear transformation from \( n \) dimensions to \( 1 \) dimension is a certain vector. 
\end{definition}

\begin{remark}
  Note then that \( \textbf{a}^{\intercal} \textbf{a} = \textbf{a} \cdot \textbf{a} = {\lVert \textbf{a} \rVert}^2 \)
\end{remark}

\begin{remark}
  \( \textbf{x} \cdot \textbf{y} \) produces a scalar, so \( \textbf{x} \cdot \textbf{y} = \textbf{x}^{\intercal} \textbf{y} \) or \( \textbf{y}^{\intercal} \textbf{x} \) (both of which are \( 1 \times 1 \) matrices), but \( \textbf{x} \cdot \textbf{y} \neq \textbf{y} \textbf{x}^{\intercal} \) or \( \textbf{x} \textbf{y}^{\intercal} \), both of which would be \( n \times n \) matrices.
\end{remark}

\begin{remark}
  The linear transformation represented by the \( n \times n \) matrix \( \textbf{a} \textbf{a}^{\intercal} \) is \( \text{proj}_{\textbf{a}} \textbf{x} \). By expressing the projection formula in terms of \( \textbf{a} \) and \( \textbf{a}^{\intercal} \), we can clearly show that it is a function in terms of \( \textbf{a} \).
\end{remark}

\begin{proof}
  \begin{align*}
    \text{proj}_{\textbf{a}} \textbf{x} &= \frac{\textbf{x} \cdot \textbf{a}}{{\lVert \textbf{a} \rVert}^2 } \textbf{a} \\
    &= \frac{\textbf{a}^{\intercal} \textbf{x}}{{\lVert \textbf{a} \rVert}^2 } \textbf{a} && \text{(express dot product as matrix mult, per second remark above})\\
    &= \textbf{a} \frac{\textbf{a}^{\intercal} \textbf{x}}{{\lVert \textbf{a} \rVert}^2 } && \text{(can move since one of the terms above is a scalar)}\\
    &= \frac{\textbf{a} \textbf{a}^{\intercal} \textbf{x}}{\textbf{a} \cdot \textbf{a}}\\
    &= \left( \frac{\textbf{a} \textbf{a}^{\intercal} } {\textbf{a}^{\intercal} \textbf{a}} \right) \textbf{x} && (\text{again, express dot product as matrix mult})\\
  \end{align*}
\end{proof}

\begin{eg}[projection expressed with transposes]
  \begin{align*}
    \text{proj}_{\begin{bmatrix}
        1 \\ 1 \\ 1
      \end{bmatrix}} &= \frac{\begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix} \begin{bmatrix} 1 & 1 & 1 \end{bmatrix}}  {\begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix} \cdot {\begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix}}} &&  \text{denom is \( 1 \times n \) times \( n \times 1 \), aka dot prod}\\  
                       &= \frac{1}{3} \begin{bmatrix}
                         1 & 1 & 1 \\
                         1 & 1 & 1 \\
                         1 & 1 & 1
                       \end{bmatrix}
  \end{align*}
\end{eg}

\begin{definition}[orthogonal matrix]
  An \( n \times n \) matrix \( A \) is orthogonal if \( A^{\intercal}A = I_n \), which is true iff \( \textbf{a}_i \cdot \textbf{a}_j = 0 \) for \( i \neq j \) and \( 1 \) for \( i=j \). 
\end{definition}

\section{Left multiplication (row vector * matrix) }

\begin{remark}
  Let \( A \) be an \( n \times m \) matrix. Let \( \textbf{x} \) be a vector of length \( n \). The operation in which a row vector is multiplied by a matrix can be expressed as \( \textbf{x}^{\intercal} \textbf{A} \). 
\end{remark}

\begin{prop}
  \( A \textbf{x} \cdot \textbf{y} = \textbf{x} \cdot \textbf{A}^{\intercal} \cdot \textbf{y}\) (To move the matrix across a dot product operator, we must transpose it)
\end{prop}

\begin{proof}
  \begin{align*}
    (A \textbf{x}) \cdot \textbf{y} &= (A \textbf{x})^{\intercal} \textbf{y} && \text{dot product as matrix mult}\\
                                    &= \textbf{x}^{\intercal} A^{\intercal} \textbf{y}\\
                                    &= \textbf{x}^{\intercal} (A^{\intercal} \textbf{y})\\
                                    &= \textbf{x} \cdot A^{\intercal} \textbf{y} 
  \end{align*}
  
\end{proof}

\section{Elementary matrices}

\begin{prop}
  Every invertible matrix can be expressed as a product of elementary matrices.
\end{prop}

\begin{proof}
  Since \( A \) is invertible, we can row reduce it to the identity matrix. We do this by multiplying \( A \) on the left by a series of elementary matrices \(E = (E_k)(\ldots)(E_2)(E_1) \) such that \( EA=I \). Elementary matrices are invertible, so we can multiply both sides of that equation by \( E^{-1} \). Then \( A = E^{-1} = ({E_1}^{-1})({E_2}^{-1})(\ldots)({E_k}^{-1}) \).
\end{proof}

\begin{remark}
  When we apply elementary operations to the rows of a matrix \( A \), we multiply \( A \) on the left by an elementary matrix \( E \), such that we get a transformed version of the rows of \( A \).
  \[
    EA = \brows{
        E_1 A \\
        E_2 A \\
        \rowsvdots \\
        E_m A
      }
  \]
\end{remark}

\begin{remark}
  We construct a row swap elementary matrix \( E \) by taking \( I_m \) and interchanging rows \( i \) and \( j \) to swap \( A_i \) and \( A_j \). A row of \( E \), \( E_k \), looks like
  \[
    \begin{cases}
      {e_k}^{\intercal}, & \text{if \( k \neq i \) or \( j \) } \\
      {e_j}^{\intercal}, & \text{if \( k=i \) } \\
      {e_i}^{\intercal}, & \text{if \( k=j \) }
    \end{cases}
  \]
  where \( e_i \) is the \( i \)-th basis vector and \( {e_i}^{\intercal} \) is the \( i \)-th basis row vector. \\
  Thus, 
  \[
    E_k A = \begin{cases}
      A_k, & \text{if \( k \neq i  \) or \( j \)} \\
      A_j, & \text{if \( k=i \)} \\
      A_i, & \text{if \( k=j \)}
    \end{cases}
  \]
\end{remark}

\end{document}



